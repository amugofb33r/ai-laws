# ai-laws


---
# Colorado Insurance Regulation 10-1-1 
## **1. What It Is — Regulation Overview**

**Colorado Insurance Regulation 10-1-1** is a **governance and risk management framework regulation** issued by the Colorado Division of Insurance (CDOI) that governs how insurers use **external consumer data and information sources (ECDIS)** and **algorithms or predictive models** that rely on that data. ([Deloitte][1])

* Originally adopted in **2023** for **life insurers** and effective **November 14, 2023**. ([Colorado Department of Insurance][2])
* **Amended August 20, 2025**, and effective **October 15, 2025**, to expand scope to additional insurance lines. ([Debevoise][3])
* The regulation is intended to ensure that the use of ECDIS and algorithmic models (including AI) is **governed, documented, tested, monitored, and controlled** to address potential unfair discrimination (especially with respect to race). ([Deloitte][1])

**ECDIS** broadly includes **external consumer data sources used to supplement or replace traditional underwriting factors or establish lifestyle indicators**—examples include credit data, social media, telematics, IoT data, and more. ([Deloitte][1])

---

## **2. Who It Applies To — Covered Entities**

The amended Regulation applies to insurers **authorized to do business in Colorado** that use ECDIS and related algorithms/predictive models in any insurance practice. ([Insure Reinsure][4])

**Covered insurer categories now include:**

* **Life Insurance Companies** (original scope). ([Deloitte][1])
* **Private Passenger Automobile Insurers** (added). ([Debevoise][3])
* **Health Benefit Plan Insurers** (added). ([Debevoise][3])

(All must comply if they use ECDIS, AI, or models based on these data.) ([Insure Reinsure][4])

---

## **3. Why It Matters — Purpose and Policy Goals**

The regulation reflects **Colorado’s effort to manage the risks of algorithmic decision-making and AI-like tools** in the insurance industry. ([PwC][5])

Key policy goals include:

* **Preventing unfair discrimination** in insurance practices based on external data and models. ([Deloitte][1])
* **Ensuring transparency and accountability** in how modern data analytics and algorithmic tools are used in underwriting, pricing, claims, or other decisions. ([PwC][5])
* Placing **governance, oversight, and risk management obligations** on insurers that use advanced data sources and decision models. ([PwC][5])

This rule also positions Colorado as a **leader among U.S. states** in regulating AI-related governance in insurance. ([PwC][5])

---

## **4. What Regulators Expect — Compliance Requirements**

Regulators expect insurers to implement and document a **robust governance and risk management framework for ECDIS, algorithms, and predictive models**. ([Justia][6])

### **Governance & Oversight**

* **Board or committee oversight** of ECDIS and models. ([PwC][5])
* **Senior management accountability** for strategy and risk. ([Justia][6])
* **Cross-functional governance groups** (e.g., legal, compliance, actuarial, data science). ([Justia][6])

### **Policies & Procedures**

* Written policies for **design, testing, deployment, monitoring, and validation** of ECDIS and models. ([Justia][6])
* Training and internal supervision programs for staff using these tools. ([Justia][6])

### **Consumer Protections**

* Processes for **handling complaints** and explaining adverse decisions linked to ECDIS/models. ([Justia][6])

### **Reporting**

* **Narrative progress reports** and **annual compliance reports** to the Division. ([Justia][7])

---

## **5. Key Risks If Unmanaged — What’s at Stake**

If insurers fail to appropriately govern, monitor, and control ECDIS and related AI models, the following risks emerge:

### **Regulatory & Legal Risk**

* **Non-compliance** may lead to enforcement actions and penalties. ([PwC][5])

### **Consumer Harm**

* Use of ECDIS/AI without controls can lead to **unfair discrimination** (especially against protected classes). ([PwC][5])

### **Reputational & Operational Risk**

* Lack of transparency or governance can erode consumer trust and invite **litigation or public scrutiny**. ([PwC][5])

### **Model Risk**

* Models that are not validated or monitored may produce **biased or inaccurate outcomes**, undermining underwriting fairness. ([PwC][5])

---

## **6. Practical Takeaway — What Insurers Should Do**

### **Get Prepared for Compliance**

* **Inventory all ECDIS and models** that rely on such data. ([PwC][5])

### **Build and Document Governance**

* Establish or formalize governance groups and board oversight that **owns AI risk management**. ([PwC][5])
* Draft clear policies for development, testing, deployment, monitoring, and review of tools. ([Justia][6])

### **Focus on Consumer Communication**

* Put processes in place to **inform customers** about adverse decisions involving ECDIS/models and how to appeal. ([Justia][6])

### **Compliance Reporting**

* Prepare for **regular reports** (interim and annual) to the Division of Insurance. ([Justia][7])

### **Quantitative Testing**

* Watch for **quantitative testing requirements** and potential future rulemaking focused on detecting bias. ([Debevoise][3])


---
# NYDFS Insurance Circular Letter No. 7 Section B

## **1) What It Is — Overview**

**NYDFS Insurance Circular Letter No. 7** is a supervisory guidance document issued by the New York State Department of Financial Services that sets out the Department’s expectations for how insurers should **develop, govern, manage, and monitor the use of Artificial Intelligence Systems (AIS) and External Consumer Data and Information Sources (ECDIS)** in the context of **insurance underwriting and pricing**. It establishes principles, governance expectations, testing and documentation requirements, and consumer transparency standards to ensure that the use of these technologies complies with applicable state and federal law and does not lead to unfair or unlawful discrimination. ([Department of Financial Services][1])

AIS is defined as machine-based systems performing functions typically associated with human intelligence (such as learning or reasoning) used to **supplement or proxy traditional underwriting or pricing**, including identifying lifestyle indicators. ECDIS is defined as external data or information used to supplement or proxy traditional underwriting or pricing. ([Department of Financial Services][1])

---

## **2) Who It Applies To — Covered Entities**

This Circular Letter **applies to all insurers authorized to write insurance in New York State**, including:

* Standard insurance companies licensed in New York,
* **Article 43 corporations**,
* **Health maintenance organizations (HMOs)**,
* **Licensed fraternal benefit societies**, and
* The **New York State Insurance Fund**. ([Department of Financial Services][1])

It is specifically aimed at those entities that use **AIS and ECDIS** in their underwriting and pricing processes. ([Department of Financial Services][1])

---

## **3) Why It Matters — Purpose and Policy Goals**

The Circular Letter is designed to advance **responsible innovation** while **protecting consumers** by ensuring that the integration of modern algorithmic and data-driven tools into insurance decision-making is safe, **fair**, and lawful. It seeks to:

* **Mitigate the risk of unfair or unlawful discrimination**, especially against protected classes, inherent in new data sources and AI systems. ([Department of Financial Services][2])
* Encourage **governance, oversight, and risk management** structures that hold insurers accountable for the outcomes of AIS and ECDIS use. ([Department of Financial Services][2])
* Ensure **transparency** with consumers about how their data is used in underwriting and pricing. ([Department of Financial Services][1])
* Promote **accuracy, reliability, and actuarial validity** of models and data sources used in insurance decisions. ([Department of Financial Services][2])

NYDFS underscores that while AIS and ECDIS can enhance efficiency and accuracy, they also raise concerns about bias, systemic inequities, and market integrity if not properly controlled. ([Department of Financial Services][1])

---

## **4) What Regulators Expect — Compliance Requirements**

NYDFS expects insurers to implement robust frameworks covering the **governance, testing, documentation, and oversight** of AIS and ECDIS. Key expectations include:

### **a. Governance and Oversight**

* Boards and senior management must **own responsibility for outcomes** of AIS and ECDIS use (not necessarily day-to-day execution). ([Department of Financial Services][1])
* A **risk-based approach** must be taken to guide the extent of oversight relative to the complexity and materiality of AIS and ECDIS use. ([Department of Financial Services][1])

### **b. Fairness and Anti-Discrimination**

* Insurers must ensure AIS and ECDIS **do not result in unfair or unlawful discrimination** against protected classes under state/federal law. ([Department of Financial Services][1])
* A **comprehensive assessment** process (quantitative and qualitative) is expected to identify discriminative effects and consider less discriminatory alternatives. ([Department of Financial Services][3])

### **c. Policies, Procedures, Documentation**

* Insurers must develop **written policies** governing AIS and ECDIS use, including roles, monitoring, reporting, and training. ([Department of Financial Services][1])
* Maintain **comprehensive documentation** of AIS models and ECDIS data sources, including testing records, model descriptions, risk assessments, and audit trails. ([Department of Financial Services][1])

### **d. Testing and Monitoring**

* Regular **testing for discrimination and model performance**, including before deployment and on an ongoing basis, is expected. ([Mayer Brown][4])

### **e. Third-Party Oversight**

* Insurers retain responsibility for AIS and ECDIS even if sourced from third-party vendors and must institute appropriate due diligence, contractual safeguards, and oversight. ([Department of Financial Services][1])

### **f. Consumer Transparency**

* Insurers must disclose to individuals that AIS/ECDIS are used, identify the data sources, and provide **reasons for adverse underwriting decisions**, including data and sources involved. ([Department of Financial Services][1])

---

## **5) Key Risks If Unmanaged**

Failing to properly manage AIS and ECDIS can expose insurers to a range of risks:

* **Regulatory and Legal Risk:** Violations of anti-discrimination laws or supervisory guidance can lead to enforcement actions or sanctions. ([Department of Financial Services][2])
* **Consumer Harm:** Unchecked biases in models or data could produce discriminatory outcomes, harming protected classes. ([Department of Financial Services][3])
* **Reputational Risk:** Poor transparency or unfair pricing decisions may erode consumer trust and brand reputation. ([Department of Financial Services][2])
* **Operational and Model Risk:** Inaccurate or poorly validated models may produce unreliable underwriting decisions, undermining risk management and pricing accuracy. ([Mayer Brown][4])

---

## **6) Practical Takeaway — What Insurers Should Do**

To align with NYDFS expectations and mitigate risk, insurers using AIS or ECDIS should take concrete steps:

### **a. Establish Solid Governance**

* Form or strengthen board and senior management oversight structures for AIS/ECDIS governance proportional to their use and risk. ([Department of Financial Services][1])

### **b. Implement Comprehensive Controls**

* Develop, document, and enforce policies and procedures covering model development, testing, validation, documentation, and lifecycle management. ([Department of Financial Services][1])

### **c. Conduct Discrimination Assessment**

* Perform rigorous quantitative and qualitative testing for unfair or unlawful discrimination and iterate to use less discriminatory alternatives when feasible. ([Department of Financial Services][3])

### **d. Maintain Transparent Consumer Communications**

* Inform applicants when AIS/ECDIS are used and provide accessible explanations for adverse underwriting decisions, including source data. ([Department of Financial Services][1])

### **e. Assess Third-Party Tools**

* Institute vendor oversight processes, including contractual rights to audits and cooperation in regulatory matters. ([Department of Financial Services][1])

By following these steps and documenting each component, insurers can better comply with NYDFS’s guidance, support fair consumer outcomes, and harness innovation responsibly. ([Department of Financial Services][2])


# NAIC “Model Bulletin on the Use of Artificial Intelligence Systems by Insurers” and the draft NAIC AI Systems Evaluation Tool
---

## **1) What It Is — Overview**

### **NAIC AI Model Bulletin**

The **NAIC Model Bulletin on the Use of Artificial Intelligence Systems by Insurers** is a **standard-setting guidance document** developed by the National Association of Insurance Commissioners. It was adopted in **December 2023** and sets expectations for how insurers should **govern, risk-manage, document, and oversee the use of AI systems** in regulated insurance practices like underwriting, pricing, claims, marketing, and other operations. ([NAIC][1])

### **NAIC AI Systems Evaluation Tool (Draft)**

The **AI Systems Evaluation Tool** is a **draft, exposure-level guidance resource** being developed by the NAIC’s Big Data and Artificial Intelligence Working Group. It complements the Model Bulletin by offering **structured, tailorable templates, surveys, checklists, and exhibits** that regulators can use to **assess an insurer’s use of AI systems and related risks** in areas including consumer impacts, governance, financial reporting, and high-risk models. ([Eversheds Sutherland][2])

Together these documents aim to move beyond high-level principles to more concrete **evaluation and oversight frameworks** for AI in insurance.

---

## **2) Who It Applies To — Covered Entities**

Both the **Model Bulletin** and the **Draft Evaluation Tool** are targeted at:

* **Insurance companies regulated by state insurance departments** within the U.S.
* Entities using **AI systems** that make or support decisions in **regulated insurance practices** (underwriting, pricing, claims, marketing, risk classification, fraud detection, etc.). ([NAIC][1])

These materials are **not binding laws** on their own, but they are intended for **adoption or adaptation by state regulators**, meaning insurers operating in jurisdictions that adopt the bulletin or use the evaluation tool will need to comply.

---

## **3) Why It Matters — Purpose and Policy Goals**

The NAIC’s AI guidance has three core goals:

### **a. Promote Responsible AI Use**

* Help the insurance industry **adopt AI safely and ethically** across processes.
* Provide a **framework for governance and documentation** of AI at scale. ([NAIC][1])

### **b. Protect Consumers**

* Ensure AI use does not lead to **inaccurate, arbitrary, unfairly discriminatory, or opaque outcomes** that could harm policyholders.
* Encourage transparency about how AI affects decisions. ([S&P Global][3])

### **c. Support Regulatory Oversight**

* Equip state insurance regulators with tools and expectations to **evaluate AI risks**, including consumer and financial risks, and **ensure compliance with existing insurance laws** (e.g., unfair trade practices). ([Eversheds Sutherland][2])

The Evaluation Tool specifically aims to help regulators **identify and assess the use and impact of AI systems** in a standardized way across carriers. ([Eversheds Sutherland][2])

---

## **4) What Regulators Expect — Compliance Requirements**

### **From the NAIC Model Bulletin**

Regulators distributing or adopting the bulletin expect insurers to develop and maintain a **written AI Systems Program** (AIS Program) that includes:

* **Governance frameworks** with clear accountabilities for oversight throughout the AI lifecycle.
* **Risk management and internal controls** designed to identify and mitigate adverse consumer outcomes.
* **Policies and procedures** for AI deployment, documentation, monitoring, and testing.
* **Third-party vendor oversight** ensuring due diligence, contractual protections, and performance monitoring for externally sourced AI.
* **Consumer notice and transparency mechanisms** about AI’s role in decisions that affect them. ([NAIC][1])

### **From the Draft Evaluation Tool**

Though still in draft form, the Evaluation Tool provides regulators with optional **questionnaires and exhibits** to collect information such as:

* **Inventory of AI systems** used and their scope.
* **Narratives and checklists on governance and risk frameworks** relative to insurer’s AI use.
* **Detailed disclosures for “high-risk” AI models** that could cause consumer or financial harm.
* **Data details** about sources and use of data in AI systems.
  These are meant to standardize **regulatory review and risk assessment** across companies. ([Eversheds Sutherland][2])

---

## **5) Key Risks If Unmanaged**

Failing to proactively manage AI use exposes insurers to several risks:

### **Consumer Harm**

* AI systems could produce **biased, opaque, or unfair outcomes**, especially impacting protected classes or vulnerable populations. ([S&P Global][3])

### **Regulatory and Legal Risk**

* Misuse or poor governance of AI could lead to **violations of insurance laws**, including unfair trade practices or discriminatory practices, and trigger enforcement actions. ([NAIC][1])

### **Operational and Model Risk**

* Lack of robust testing, documentation, and oversight increases the likelihood of **inaccurate, inconsistent, or unreliable outputs**, undermining pricing accuracy and business decisions. ([S&P Global][3])

### **Reputation Risk**

* Poorly governed AI can erode **consumer trust** and attract negative scrutiny from regulators, consumers, or public stakeholders. ([S&P Global][3])

---

## **6) Practical Takeaway — What Insurers Should Do**

Here are practical steps insurers should take now:

### **Establish Comprehensive AI Governance**

* Create a **written AIS Program** covering AI inventory, policies, roles, oversight, and lifecycle management.
* Formalize senior leadership and board accountability for AI risks. ([NAIC][1])

### **Implement Strong Risk Management**

* Build frameworks for **validation, bias testing, documentation, and monitoring** of AI systems.
* Integrate internal controls tailored to the scale and complexity of AI use, including **vendor and third-party oversight**. ([NAIC][1])

### **Prepare for Regulatory Evaluation**

* Maintain clear records that a regulator could request, including inventories, governance descriptions, risk assessments, and consumer impacts.
* Use the **AI Systems Evaluation Tool’s structures** as internal checklists to self-assess readiness and risk exposure ahead of regulator inquiries. ([Eversheds Sutherland][2])

### **Communicate with Consumers**

* Provide appropriate **notices and explanations** when AI affects decisions that impact customers.
* Ensure consumer explanations are understandable and meet regulatory expectations. ([NAIC][1])

### **Stay Aligned with State Adoption**

* Track which states have **adopted the Model Bulletin** or are using the evaluation tool and adapt compliance strategies accordingly. ([NAIC][4])

---
# State Insurance Department Bulletins on AI
---

## **1) What It Is — Overview**

**State Bulletins on AI in Insurance** are supervisory guidance documents issued by state insurance regulators (e.g., Delaware, Wisconsin, Iowa, Oklahoma, Connecticut) that **outline expectations for insurers’ use of artificial intelligence (AI) systems and technologies** in insurance practices. They are largely based on the **NAIC Model Bulletin on the Use of Artificial Intelligence Systems by Insurers**, adopted in December 2023, and adapted by individual states.([NAIC][1])

These bulletins remind insurers that **decisions or actions impacting consumers made or supported by AI must comply with all applicable insurance laws and regulations** and may be subject to regulatory review or market conduct examination. They apply whether the AI involves predictive models, automated pricing, claims processing, generative imagery, image-based risk scoring, or other AI outputs.([OCI][2])

---

## **2) Who It Applies To — Covered Entities**

State AI bulletins generally apply to:

* **All insurers authorized to do business in the state** (life, property/casualty, health).([NAIC][1])
* **Insurance carriers using AI systems** in any regulated practice, including underwriting, pricing/rating, marketing, claims, and risk assessment.([OCI][2])
* In many states, guidance also applies to **third-party vendors and models used on behalf of insurers**, with expectations for due diligence and oversight.([McDermott][3])

Unlike laws that single out specific AI applications (e.g., California health AI restrictions), these bulletins are broad statements of regulatory expectations for *all AI use cases*, including generative imagery models if those influence decision-making.([NAIC][1])

---

## **3) Why It Matters — Purpose and Policy Goals**

State AI bulletins are intended to:

**Promote Responsible AI Use**

* Encourage insurers to deploy AI in ways that are **fair, transparent, and accountable**.([Skadden][4])

**Protect Consumers**

* Ensure that AI tools — including those that generate or interpret images (e.g., for visual risk assessment) — do **not produce inaccurate, biased, opaque, or discriminatory outcomes**.([OCI][2])

**Ensure Legal Compliance**

* Remind insurers that **existing insurance laws** (unfair trade practices, discrimination, claims practices, ratemaking) apply regardless of whether an AI or imagery model is involved.([NAIC][5])

**Support Regulatory Oversight**

* Provide regulators with a basis to **request documentation and examine AI governance**, testing, and outcomes during market conduct reviews or investigations.([McDermott][3])

In short, the bulletins aim to *balance innovation with consumer protection and legal compliance* as AI becomes more deeply embedded in insurance operations.([State of Delaware News][6])

---

## **4) What Regulators Expect — Compliance Requirements**

Though not laws themselves, these bulletins set **clear expectations** for insurer behavior:

### **Governance & Written AI Program**

Insurers are expected to **develop, implement, and maintain a documented AI systems program** (often called an *AIS Program*) that addresses:

* Governance (board/senior management oversight)
* Risk management controls
* Internal audit functions
* Lifecycle management (design, validation, deployment, monitoring) of AI systems.([NAIC][5])

### **Risk Mitigation and Testing**

* AI systems must be tested for **accuracy, bias, and fairness**, with controls proportionate to their potential impact.
* Models involving imagery (e.g., visual risk scores) should fall under the same evaluation expectations if they affect underwriting or pricing decisions.([McDermott][3])

### **Legal and Regulatory Compliance**

* Decisions supported by AI must comply with **state unfair trade practice, claims, and ratemaking laws** and **anti-discrimination standards**.([Taft Law][7])

### **Documentation and Examinations**

* Regulators may request documentation on AI governance, third-party due diligence, validation results, consumer impacts, and controls during market conduct exams.([OCI][2])

### **Consumer Transparency**

* Bulletins encourage insurers to **inform consumers** when AI is used in decision-making and to be prepared to explain how models (including imagery or generative tools) influence outcomes.([NAIC][5])

---

## **5) Key Risks If Unmanaged**

If insurers do not responsibly govern AI — including any AI that generates or uses imagery for decisions — they face several risks:

**Consumer Harm**

* Bias or errors in AI-generated content or image interpretation can lead to **inaccurate pricing, unfair non-renewals, or discriminatory outcomes**.([Carlton Fields][8])

**Regulatory and Legal Risk**

* Violations of state insurance laws (even if indirect via AI tools) can trigger **market conduct actions, fines, or corrective orders**.([OCI][2])

**Reputational and Operational Risk**

* Lack of transparency or explanation of AI decisions may erode **customer trust** and invite scrutiny from regulators or the public.([McDermott][3])

**Model Risk**

* Inadequately validated AI — including generative or image-based models — can degrade decision quality over time (e.g., due to data drift), harming risk assessment accuracy.([NAIC][5])

---

## **6) Practical Takeaway — What Insurers Should Do**

To align with state bulletins and mitigate risks, insurers should:

### **Build a Robust AI Governance Framework**

* Establish a formal **AIS Program** that includes governance, policies, procedures, and oversight mechanisms.([McDermott][3])

### **Inventory and Monitor AI Systems**

* Identify all AI tools in use — including those involving imagery (e.g., for property risk assessment) — and evaluate them for consumer impact and compliance.([NAIC][5])

### **Test and Validate**

* Conduct **bias, accuracy, and fairness testing**, especially for systems that influence consumer outcomes. Document validation and ongoing monitoring.([McDermott][3])

### **Vendor and Third-Party Oversight**

* Ensure that third-party AI solutions (including image analytics providers) are covered by due diligence, contractual safeguards, and monitoring.([McDermott][3])

### **Prepare for Regulatory Requests**

* Maintain documentation ready for examiners, including governance structures, program controls, test results, and records of consumer communications.([OCI][2])

### **Enhance Transparency**

* Be ready to explain how AI systems — including those that use or generate visual data — affect decisions, and consider consumer notices where appropriate.([NAIC][5])

---

**In summary:** While there isn’t a set of bulletins *specifically* on “AI imagery,” **state AI bulletins address insurer use of all AI systems (including image-related models)** and reinforce that these systems must be governed, tested, documented, and compliant with existing insurance laws to protect consumers and support fair outcomes.([NAIC][1])

---



